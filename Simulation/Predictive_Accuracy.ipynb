{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b295788e",
   "metadata": {},
   "source": [
    "# Predictive Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7dc8e5",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8e12094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "# GPBoost\n",
    "import gpboost as gpb\n",
    "\n",
    "# For CRPS & Log-Score\n",
    "from scipy.stats import norm\n",
    "from scipy.special import erf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32b88b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056de8f8",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65cfe777-58ae-41a0-97e9-b3ab1eb35191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import gpytorch\n",
    "#from gpytorch.means import ConstantMean\n",
    "#from gpytorch.kernels import ScaleKernel, MaternKernel, InducingPointKernel\n",
    "#from gpytorch.distributions import MultivariateNormal\n",
    "#from gpytorch.models import ApproximateGP\n",
    "#from gpytorch.variational import CholeskyVariationalDistribution\n",
    "#from gpytorch.variational import VariationalStrategy\n",
    "# Number of input dimensions\n",
    "#vector_d = [2,10]\n",
    "## number of reps\n",
    "#num_reps = 10\n",
    "#for i, val1 in enumerate(vector_d):\n",
    "\n",
    "#    # Ranges\n",
    "#    ranges = torch.tensor([(0.15, 0.3,0.45,0.6,0.75)])\n",
    "#    avg_dist_5 = np.sqrt(5/6)\n",
    "#    if val1 != 5:\n",
    "#        # average distance\n",
    "#        avg_dist = np.sqrt(val1/6)\n",
    "#        min_ratio = avg_dist_5/0.15\n",
    "#        max_ratio = avg_dist_5/0.75\n",
    "#        min_range = avg_dist/min_ratio\n",
    "#        max_range = avg_dist/max_ratio\n",
    "#        ranges = torch.linspace(min_range, max_range, val1)\n",
    "#        ranges = ranges.unsqueeze(0)\n",
    "        \n",
    "#    ranges = 2.448/3 * ranges # Matern05\n",
    "#    ranges = 2.448/2.74 * ranges # Matern15\n",
    "#    ranges = 2.448/2.647 * ranges\n",
    "#    ranges = ranges # Gaussian\n",
    "#    print(ranges)\n",
    "#    for j in range(num_reps):\n",
    "#        print(j)\n",
    "#        X, y, _ = simulate_gp_response(\"matern\",40000, 0.3, 1.0, ranges,j,1.5)\n",
    "#        data = pd.DataFrame(X.numpy(), columns=[f\"x{i+1}\" for i in range(X.shape[1])])\n",
    "#        data['y'] = y.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489ccf9e",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49c0f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRPS\n",
    "def crps_norm_vectorized(observations, means, sigmas):\n",
    "    \"\"\"\n",
    "    Compute CRPS for multiple normal distributions.\n",
    "    \n",
    "    Parameters:\n",
    "    observations (array-like): Observed values\n",
    "    means (array-like): Means of the forecast distributions\n",
    "    sigmas (array-like): Variance of the forecast distributions\n",
    "    \n",
    "    Returns:\n",
    "    array-like: CRPS values\n",
    "    \"\"\"\n",
    "    observations = np.asarray(observations)\n",
    "    means = np.asarray(means)\n",
    "    sigmas = np.sqrt(np.asarray(sigmas))\n",
    "    sigmas = np.where((sigmas == 0) | np.isnan(sigmas), 1e-06, sigmas)\n",
    "    z = (observations - means) / sigmas\n",
    "    crps = sigmas * (z * (2 * norm.cdf(z) - 1) + 2 * norm.pdf(z) - 1 / np.sqrt(np.pi))\n",
    "    return np.mean(crps)\n",
    "\n",
    "# Log-Score\n",
    "def log_score_norm_vectorized(observations, means, sigmas):\n",
    "    \"\"\"\n",
    "    Compute Log Score for multiple normal distributions.\n",
    "    \n",
    "    Parameters:\n",
    "    observations (array-like): Observed values\n",
    "    means (array-like): Means of the forecast distributions\n",
    "    sigmas (array-like): Variance of the forecast distributions\n",
    "    \n",
    "    Returns:\n",
    "    array-like: Log Score values\n",
    "    \"\"\"\n",
    "    observations = np.asarray(observations)\n",
    "    means = np.asarray(means)\n",
    "    sigmas = np.sqrt(np.asarray(sigmas))\n",
    "    sigmas = np.where((sigmas == 0) | np.isnan(sigmas), 1e-06, sigmas)\n",
    "    return np.mean(-norm.logpdf(observations, means, sigmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32873d44",
   "metadata": {},
   "source": [
    "### Matern05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7d8cf85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0774, 0.3871]])\n",
      "0\n",
      "torch.Size([10000, 2])\n",
      "torch.Size([9916, 2])\n",
      "torch.Size([10000, 2])\n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "1\n",
      "torch.Size([10000, 2])\n",
      "torch.Size([9967, 2])\n",
      "torch.Size([10000, 2])\n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "tensor([[0.1731, 0.2500, 0.3270, 0.4039, 0.4808, 0.5578, 0.6347, 0.7116, 0.7886,\n",
      "         0.8655]])\n",
      "0\n",
      "torch.Size([10000, 10])\n",
      "torch.Size([10015, 10])\n",
      "torch.Size([10000, 10])\n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "1\n",
      "torch.Size([10000, 10])\n",
      "torch.Size([10135, 10])\n",
      "torch.Size([10000, 10])\n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n"
     ]
    }
   ],
   "source": [
    "# number of inducing points\n",
    "num_ind_points = 200\n",
    "# Different number of Vecchia neighbors\n",
    "num_vecchia_neighbors = 30\n",
    "# Number of input dimensions\n",
    "vector_d = [2,10]\n",
    "# number of reps\n",
    "num_reps = 10\n",
    "if toy:\n",
    "    num_reps = 2\n",
    "# Define row and column names\n",
    "row_names = ['2-d','10-d']\n",
    "col_names = ['Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6',\n",
    "             'Iteration 7', 'Iteration 8', 'Iteration 9', 'Iteration 10']\n",
    "\n",
    "# List of matrix names\n",
    "matrix_names = [\n",
    "    \"matrix_t_vecchia\", \"matrix_t_fitc\", \"matrix_t_fsva_corr\", \"matrix_t_fsva_eucl\",\n",
    "    \"matrix_tp_vecchia\", \"matrix_tp_fitc\", \"matrix_tp_fsva_corr\", \"matrix_tp_fsva_eucl\",\n",
    "    \"matrix_RMSE_vecchia\", \"matrix_RMSE_fitc\", \"matrix_RMSE_fsva_corr\", \"matrix_RMSE_fsva_eucl\",\n",
    "    \"matrix_LS_vecchia\", \"matrix_LS_fitc\", \"matrix_LS_fsva_corr\", \"matrix_LS_fsva_eucl\",\n",
    "    \"matrix_CRPS_vecchia\", \"matrix_CRPS_fitc\", \"matrix_CRPS_fsva_corr\", \"matrix_CRPS_fsva_eucl\",\n",
    "    \"matrix_RMSE_exp_vecchia\", \"matrix_RMSE_exp_fitc\", \"matrix_RMSE_exp_fsva_corr\", \"matrix_RMSE_exp_fsva_eucl\",\n",
    "    \"matrix_LS_exp_vecchia\", \"matrix_LS_exp_fitc\", \"matrix_LS_exp_fsva_corr\", \"matrix_LS_exp_fsva_eucl\",\n",
    "    \"matrix_CRPS_exp_vecchia\", \"matrix_CRPS_exp_fitc\", \"matrix_CRPS_exp_fsva_corr\", \"matrix_CRPS_exp_fsva_eucl\"\n",
    "]\n",
    "\n",
    "# Create empty matrices and assign them dynamically to the global scope\n",
    "for matrix_name in matrix_names:\n",
    "    globals()[matrix_name] = pd.DataFrame(np.zeros((len(row_names), len(col_names))), \n",
    "                      index=row_names, columns=col_names)\n",
    "\n",
    "# Nested loop to iterate over both vectors\n",
    "for i, val1 in enumerate(vector_d):\n",
    "    # Ranges\n",
    "    ranges = torch.tensor([(0.15, 0.3,0.45,0.6,0.75)])\n",
    "    avg_dist_5 = np.sqrt(5/6)\n",
    "    if val1 != 5:\n",
    "        # average distance\n",
    "        avg_dist = np.sqrt(val1/6)\n",
    "        min_ratio = avg_dist_5/0.15\n",
    "        max_ratio = avg_dist_5/0.75\n",
    "        min_range = avg_dist/min_ratio\n",
    "        max_range = avg_dist/max_ratio\n",
    "        ranges = torch.linspace(min_range, max_range, val1)\n",
    "        ranges = ranges.unsqueeze(0)\n",
    "    ranges = 2.448/3 * ranges\n",
    "    print(ranges)\n",
    "    flattened = np.asarray(ranges).reshape(-1)\n",
    "    result = np.concatenate(([0.3, 1], flattened))\n",
    "    for j in range(num_reps):\n",
    "        print(j)\n",
    "        url = f\"https://raw.githubusercontent.com/TimGyger/VIF/refs/heads/main/Simulation/Data/Matern05/simulated_data_Matern05_{val1}_{j}.txt\"\n",
    "\n",
    "        # Load the CSV file directly from the URL\n",
    "        df = pd.read_csv(url)\n",
    "\n",
    "        # Select the first d columns for X\n",
    "        X = df.iloc[:, :val1]  # First d columns\n",
    "\n",
    "        # Select the last column for y\n",
    "        y = df.iloc[:, -1]  # Last column\n",
    "        \n",
    "        # Construct Test and Train sets \n",
    "        corner = 1 - 1/(4**(1/val1))\n",
    "        X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        y = torch.tensor(y.values, dtype=torch.float32)\n",
    "        upper_corner_mask = torch.all(X > corner, dim = 1)\n",
    "        \n",
    "        X_exp_test = X[upper_corner_mask]\n",
    "        y_exp_test =  y[upper_corner_mask]\n",
    "        X_remaining = X[~upper_corner_mask]\n",
    "        y_remaining = y[~upper_corner_mask]\n",
    "        X_int_test = X_remaining[:10000]\n",
    "        y_int_test = y_remaining[:10000]\n",
    "        X_train = X_remaining[10000:20000]\n",
    "        y_train = y_remaining[10000:20000]\n",
    "        \n",
    "        print(X_train.size())\n",
    "        print(X_exp_test.size())\n",
    "        print(X_int_test.size())\n",
    "        \n",
    "        \n",
    "        X_train = np.asarray(X_train)\n",
    "        y_train = np.asarray(y_train)\n",
    "        X_int_test = np.asarray(X_int_test)\n",
    "        y_int_test = np.asarray(y_int_test)\n",
    "        X_exp_test = np.asarray(X_exp_test)\n",
    "        y_exp_test = np.asarray(y_exp_test)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # Vecchia\n",
    "        model_vecchia = gpb.GPModel(gp_coords=X_train, cov_function=\"matern_ard\", cov_fct_shape = 0.5,\n",
    "                                 likelihood=\"gaussian\",num_neighbors = num_vecchia_neighbors,\n",
    "                                 matrix_inversion_method = \"cholesky\", gp_approx=\"vecchia\",seed = 10)\n",
    "        model_vecchia.set_optim_params(params={\"init_cov_pars\": result})\n",
    "        model_vecchia.fit(y = y_train)\n",
    "        # End the timer\n",
    "        end_time = time.time()\n",
    "        matrix_t_vecchia.loc[row_names[i], col_names[j]] = end_time - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        pred_vecchia = model_vecchia.predict(gp_coords_pred = X_int_test, predict_var = True)\n",
    "        end_time = time.time()\n",
    "        matrix_tp_vecchia.loc[row_names[i], col_names[j]] = end_time - start_time\n",
    "        matrix_RMSE_vecchia.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_int_test - pred_vecchia['mu'])))\n",
    "        matrix_CRPS_vecchia.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_int_test, pred_vecchia['mu'], pred_vecchia['var'])\n",
    "        matrix_LS_vecchia.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_int_test, pred_vecchia['mu'], pred_vecchia['var'])\n",
    "\n",
    "        pred_vecchia_exp = model_vecchia.predict(gp_coords_pred = X_exp_test, predict_var = True)\n",
    "\n",
    "        matrix_RMSE_exp_vecchia.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_exp_test - pred_vecchia_exp['mu'])))\n",
    "        matrix_CRPS_exp_vecchia.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_exp_test, pred_vecchia_exp['mu'], pred_vecchia_exp['var'])\n",
    "        matrix_LS_exp_vecchia.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_exp_test, pred_vecchia_exp['mu'], pred_vecchia_exp['var'])\n",
    "\n",
    "        start_time = time.time()\n",
    "        # FITC\n",
    "        model_FITC = gpb.GPModel(gp_coords=X_train, cov_function=\"matern_ard\", cov_fct_shape = 0.5,\n",
    "                                 likelihood=\"gaussian\",num_ind_points = 200,ind_points_selection = \"kmeans++\",\n",
    "                                 matrix_inversion_method = \"cholesky\", gp_approx=\"fitc\",seed = 10)\n",
    "        \n",
    "        model_FITC.set_optim_params(params={\"init_cov_pars\": result})\n",
    "        \n",
    "        model_FITC.fit(y = y_train)\n",
    "        # End the timer\n",
    "        end_time = time.time()\n",
    "        matrix_t_fitc.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        pred_fitc = model_FITC.predict(gp_coords_pred = X_int_test, predict_var = True)\n",
    "        end_time = time.time()\n",
    "        matrix_tp_fitc.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "        matrix_RMSE_fitc.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_int_test - pred_fitc['mu'])))\n",
    "        matrix_CRPS_fitc.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_int_test, pred_fitc['mu'], pred_fitc['var'])\n",
    "        matrix_LS_fitc.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_int_test, pred_fitc['mu'], pred_fitc['var'])\n",
    "\n",
    "        pred_fitc_exp = model_FITC.predict(gp_coords_pred = X_exp_test, predict_var = True)\n",
    "\n",
    "        matrix_RMSE_exp_fitc.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_exp_test - pred_fitc_exp['mu'])))\n",
    "        matrix_CRPS_exp_fitc.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_exp_test, pred_fitc_exp['mu'], pred_fitc_exp['var'])\n",
    "        matrix_LS_exp_fitc.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_exp_test, pred_fitc_exp['mu'], pred_fitc_exp['var'])\n",
    "\n",
    "        start_time = time.time()\n",
    "        # FSVA_eucl\n",
    "        model_FSVA_eucl = gpb.GPModel(gp_coords=X_train, cov_function=\"matern_ard\", cov_fct_shape = 0.5,num_neighbors = num_vecchia_neighbors,\n",
    "                                 likelihood=\"gaussian\",num_ind_points = num_ind_points,ind_points_selection = \"kmeans++\",\n",
    "                                 matrix_inversion_method = \"cholesky\", gp_approx=\"vecchia\",seed = 2)\n",
    "        model_FSVA_eucl.set_optim_params(params={\"init_cov_pars\": result})\n",
    "        model_FSVA_eucl.fit(y = y_train)\n",
    "        # End the timer\n",
    "        end_time = time.time()\n",
    "        matrix_t_fsva_eucl.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        pred_fsva_eucl = model_FSVA_eucl.predict(gp_coords_pred = X_int_test, predict_var = True)\n",
    "        end_time = time.time()\n",
    "        matrix_tp_fsva_eucl.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "        matrix_RMSE_fsva_eucl.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_int_test - pred_fsva_eucl['mu'])))\n",
    "        matrix_CRPS_fsva_eucl.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_int_test, pred_fsva_eucl['mu'], pred_fsva_eucl['var'])\n",
    "        matrix_LS_fsva_eucl.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_int_test, pred_fsva_eucl['mu'], pred_fsva_eucl['var'])\n",
    "\n",
    "        pred_fsva_eucl_exp = model_FSVA_eucl.predict(gp_coords_pred = X_exp_test, predict_var = True)\n",
    "\n",
    "        matrix_RMSE_exp_fsva_eucl.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_exp_test - pred_fsva_eucl_exp['mu'])))\n",
    "        matrix_CRPS_exp_fsva_eucl.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_exp_test, pred_fsva_eucl_exp['mu'], pred_fsva_eucl_exp['var'])\n",
    "        matrix_LS_exp_fsva_eucl.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_exp_test, pred_fsva_eucl_exp['mu'], pred_fsva_eucl_exp['var'])\n",
    "\n",
    "        \n",
    "        \n",
    "        start_time = time.time()\n",
    "        # FSVA_corr\n",
    "        model_FSVA_corr = gpb.GPModel(gp_coords=X_train, cov_function=\"matern_ard\", cov_fct_shape = 0.5,num_neighbors = num_vecchia_neighbors,\n",
    "                                 likelihood=\"gaussian\",num_ind_points = num_ind_points,ind_points_selection = \"kmeans++\",\n",
    "                                 matrix_inversion_method = \"cholesky\", gp_approx=\"vecchia\",seed = 4)\n",
    "        model_FSVA_corr.set_optim_params(params={\"init_cov_pars\": result})\n",
    "        model_FSVA_corr.fit(y = y_train)\n",
    "        # End the timer\n",
    "        end_time = time.time()\n",
    "        matrix_t_fsva_corr.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        pred_fsva_corr = model_FSVA_corr.predict(gp_coords_pred = X_int_test, predict_var = True)\n",
    "        end_time = time.time()\n",
    "        matrix_tp_fsva_corr.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "        matrix_RMSE_fsva_corr.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_int_test - pred_fsva_corr['mu'])))\n",
    "        matrix_CRPS_fsva_corr.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_int_test, pred_fsva_corr['mu'], pred_fsva_corr['var'])\n",
    "        matrix_LS_fsva_corr.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_int_test, pred_fsva_corr['mu'], pred_fsva_corr['var'])\n",
    "\n",
    "        pred_fsva_corr_exp = model_FSVA_corr.predict(gp_coords_pred = X_exp_test, predict_var = True)\n",
    "\n",
    "        matrix_RMSE_exp_fsva_corr.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_exp_test - pred_fsva_corr_exp['mu'])))\n",
    "        matrix_CRPS_exp_fsva_corr.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_exp_test, pred_fsva_corr_exp['mu'], pred_fsva_corr_exp['var'])\n",
    "        matrix_LS_exp_fsva_corr.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_exp_test, pred_fsva_corr_exp['mu'], pred_fsva_corr_exp['var'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa60f1fd",
   "metadata": {},
   "source": [
    "### Matern 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19976035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0848, 0.4238]])\n",
      "0\n",
      "torch.Size([10000, 2])\n",
      "torch.Size([9916, 2])\n",
      "torch.Size([10000, 2])\n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "1\n",
      "torch.Size([10000, 2])\n",
      "torch.Size([9967, 2])\n",
      "torch.Size([10000, 2])\n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "tensor([[0.1895, 0.2738, 0.3580, 0.4422, 0.5265, 0.6107, 0.6949, 0.7792, 0.8634,\n",
      "         0.9476]])\n",
      "0\n",
      "torch.Size([10000, 10])\n",
      "torch.Size([10015, 10])\n",
      "torch.Size([10000, 10])\n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "1\n",
      "torch.Size([10000, 10])\n",
      "torch.Size([10135, 10])\n",
      "torch.Size([10000, 10])\n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n"
     ]
    }
   ],
   "source": [
    "# number of inducing points\n",
    "num_ind_points = 200\n",
    "# Different number of Vecchia neighbors\n",
    "num_vecchia_neighbors = 30\n",
    "# Number of input dimensions\n",
    "vector_d = [2,10]\n",
    "# number of reps\n",
    "num_reps = 10\n",
    "if toy:\n",
    "    num_reps = 2\n",
    "# Define row and column names\n",
    "row_names = ['2-d','5-d','10-d','20-d','50-d','100-d']\n",
    "col_names = ['Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6',\n",
    "             'Iteration 7', 'Iteration 8', 'Iteration 9', 'Iteration 10']\n",
    "if toy:\n",
    "    row_names = ['2-d','10-d']\n",
    "# List of matrix names\n",
    "matrix_names = [\n",
    "    \"matrix_t_vecchia\", \"matrix_t_fitc\", \"matrix_t_fsva_corr\", \"matrix_t_fsva_eucl\",\n",
    "    \"matrix_tp_vecchia\", \"matrix_tp_fitc\", \"matrix_tp_fsva_corr\", \"matrix_tp_fsva_eucl\",\n",
    "    \"matrix_RMSE_vecchia\", \"matrix_RMSE_fitc\", \"matrix_RMSE_fsva_corr\", \"matrix_RMSE_fsva_eucl\",\n",
    "    \"matrix_LS_vecchia\", \"matrix_LS_fitc\", \"matrix_LS_fsva_corr\", \"matrix_LS_fsva_eucl\",\n",
    "    \"matrix_CRPS_vecchia\", \"matrix_CRPS_fitc\", \"matrix_CRPS_fsva_corr\", \"matrix_CRPS_fsva_eucl\",\n",
    "    \"matrix_RMSE_exp_vecchia\", \"matrix_RMSE_exp_fitc\", \"matrix_RMSE_exp_fsva_corr\", \"matrix_RMSE_exp_fsva_eucl\",\n",
    "    \"matrix_LS_exp_vecchia\", \"matrix_LS_exp_fitc\", \"matrix_LS_exp_fsva_corr\", \"matrix_LS_exp_fsva_eucl\",\n",
    "    \"matrix_CRPS_exp_vecchia\", \"matrix_CRPS_exp_fitc\", \"matrix_CRPS_exp_fsva_corr\", \"matrix_CRPS_exp_fsva_eucl\"\n",
    "]\n",
    "\n",
    "# Create empty matrices and assign them dynamically to the global scope\n",
    "for matrix_name in matrix_names:\n",
    "    globals()[matrix_name] = pd.DataFrame(np.zeros((len(row_names), len(col_names))), \n",
    "                      index=row_names, columns=col_names)\n",
    "\n",
    "# Nested loop to iterate over both vectors\n",
    "for i, val1 in enumerate(vector_d):\n",
    "    # Ranges\n",
    "    ranges = torch.tensor([(0.15, 0.3,0.45,0.6,0.75)])\n",
    "    avg_dist_5 = np.sqrt(5/6)\n",
    "    if val1 != 5:\n",
    "        # average distance\n",
    "        avg_dist = np.sqrt(val1/6)\n",
    "        min_ratio = avg_dist_5/0.15\n",
    "        max_ratio = avg_dist_5/0.75\n",
    "        min_range = avg_dist/min_ratio\n",
    "        max_range = avg_dist/max_ratio\n",
    "        ranges = torch.linspace(min_range, max_range, val1)\n",
    "        ranges = ranges.unsqueeze(0)\n",
    "    ranges = 2.448/2.74 * ranges\n",
    "    print(ranges)\n",
    "    flattened = np.asarray(ranges).reshape(-1)\n",
    "    result = np.concatenate(([0.3, 1], flattened))\n",
    "    for j in range(num_reps):\n",
    "        print(j)\n",
    "        url = f\"https://raw.githubusercontent.com/TimGyger/VIF/refs/heads/main/Simulation/Data/Matern15/simulated_data_Matern15_{val1}_{j}.txt\"\n",
    "\n",
    "        # Load the CSV file directly from the URL\n",
    "        df = pd.read_csv(url)\n",
    "\n",
    "        # Select the first d columns for X\n",
    "        X = df.iloc[:, :val1]  # First d columns\n",
    "\n",
    "        # Select the last column for y\n",
    "        y = df.iloc[:, -1]  # Last column\n",
    "        \n",
    "        # Construct Test and Train sets \n",
    "        corner = 1 - 1/(4**(1/val1))\n",
    "        X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        y = torch.tensor(y.values, dtype=torch.float32)\n",
    "        upper_corner_mask = torch.all(X > corner, dim = 1)\n",
    "        \n",
    "        X_exp_test = X[upper_corner_mask]\n",
    "        y_exp_test =  y[upper_corner_mask]\n",
    "        X_remaining = X[~upper_corner_mask]\n",
    "        y_remaining = y[~upper_corner_mask]\n",
    "        X_int_test = X_remaining[:10000]\n",
    "        y_int_test = y_remaining[:10000]\n",
    "        X_train = X_remaining[10000:20000]\n",
    "        y_train = y_remaining[10000:20000]\n",
    "        \n",
    "        print(X_train.size())\n",
    "        print(X_exp_test.size())\n",
    "        print(X_int_test.size())\n",
    "        \n",
    "        \n",
    "        X_train = np.asarray(X_train)\n",
    "        y_train = np.asarray(y_train)\n",
    "        X_int_test = np.asarray(X_int_test)\n",
    "        y_int_test = np.asarray(y_int_test)\n",
    "        X_exp_test = np.asarray(X_exp_test)\n",
    "        y_exp_test = np.asarray(y_exp_test)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # Vecchia\n",
    "        model_vecchia = gpb.GPModel(gp_coords=X_train, cov_function=\"matern_ard\", cov_fct_shape = 1.5,\n",
    "                                 likelihood=\"gaussian\",num_neighbors = num_vecchia_neighbors,\n",
    "                                 matrix_inversion_method = \"cholesky\", gp_approx=\"vecchia\",seed = 10)\n",
    "        model_vecchia.set_optim_params(params={\"init_cov_pars\": result})\n",
    "        model_vecchia.fit(y = y_train)\n",
    "        # End the timer\n",
    "        end_time = time.time()\n",
    "        matrix_t_vecchia.loc[row_names[i], col_names[j]] = end_time - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        pred_vecchia = model_vecchia.predict(gp_coords_pred = X_int_test, predict_var = True)\n",
    "        end_time = time.time()\n",
    "        matrix_tp_vecchia.loc[row_names[i], col_names[j]] = end_time - start_time\n",
    "        matrix_RMSE_vecchia.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_int_test - pred_vecchia['mu'])))\n",
    "        matrix_CRPS_vecchia.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_int_test, pred_vecchia['mu'], pred_vecchia['var'])\n",
    "        matrix_LS_vecchia.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_int_test, pred_vecchia['mu'], pred_vecchia['var'])\n",
    "\n",
    "        pred_vecchia_exp = model_vecchia.predict(gp_coords_pred = X_exp_test, predict_var = True)\n",
    "\n",
    "        matrix_RMSE_exp_vecchia.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_exp_test - pred_vecchia_exp['mu'])))\n",
    "        matrix_CRPS_exp_vecchia.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_exp_test, pred_vecchia_exp['mu'], pred_vecchia_exp['var'])\n",
    "        matrix_LS_exp_vecchia.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_exp_test, pred_vecchia_exp['mu'], pred_vecchia_exp['var'])\n",
    "\n",
    "        start_time = time.time()\n",
    "        # FITC\n",
    "        model_FITC = gpb.GPModel(gp_coords=X_train, cov_function=\"matern_ard\", cov_fct_shape = 1.5,\n",
    "                                 likelihood=\"gaussian\",num_ind_points = 200,ind_points_selection = \"kmeans++\",\n",
    "                                 matrix_inversion_method = \"cholesky\", gp_approx=\"fitc\",seed = 10)\n",
    "        \n",
    "        model_FITC.set_optim_params(params={\"init_cov_pars\": result})\n",
    "        \n",
    "        model_FITC.fit(y = y_train)\n",
    "        # End the timer\n",
    "        end_time = time.time()\n",
    "        matrix_t_fitc.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        pred_fitc = model_FITC.predict(gp_coords_pred = X_int_test, predict_var = True)\n",
    "        end_time = time.time()\n",
    "        matrix_tp_fitc.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "        matrix_RMSE_fitc.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_int_test - pred_fitc['mu'])))\n",
    "        matrix_CRPS_fitc.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_int_test, pred_fitc['mu'], pred_fitc['var'])\n",
    "        matrix_LS_fitc.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_int_test, pred_fitc['mu'], pred_fitc['var'])\n",
    "\n",
    "        pred_fitc_exp = model_FITC.predict(gp_coords_pred = X_exp_test, predict_var = True)\n",
    "\n",
    "        matrix_RMSE_exp_fitc.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_exp_test - pred_fitc_exp['mu'])))\n",
    "        matrix_CRPS_exp_fitc.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_exp_test, pred_fitc_exp['mu'], pred_fitc_exp['var'])\n",
    "        matrix_LS_exp_fitc.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_exp_test, pred_fitc_exp['mu'], pred_fitc_exp['var'])\n",
    "\n",
    "        start_time = time.time()\n",
    "        # FSVA_eucl\n",
    "        model_FSVA_eucl = gpb.GPModel(gp_coords=X_train, cov_function=\"matern_ard\", cov_fct_shape = 1.5,num_neighbors = num_vecchia_neighbors,\n",
    "                                 likelihood=\"gaussian\",num_ind_points = num_ind_points,ind_points_selection = \"kmeans++\",\n",
    "                                 matrix_inversion_method = \"cholesky\", gp_approx=\"vecchia\",seed = 2)\n",
    "        model_FSVA_eucl.set_optim_params(params={\"init_cov_pars\": result})\n",
    "        model_FSVA_eucl.fit(y = y_train)\n",
    "        # End the timer\n",
    "        end_time = time.time()\n",
    "        matrix_t_fsva_eucl.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        pred_fsva_eucl = model_FSVA_eucl.predict(gp_coords_pred = X_int_test, predict_var = True)\n",
    "        end_time = time.time()\n",
    "        matrix_tp_fsva_eucl.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "        matrix_RMSE_fsva_eucl.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_int_test - pred_fsva_eucl['mu'])))\n",
    "        matrix_CRPS_fsva_eucl.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_int_test, pred_fsva_eucl['mu'], pred_fsva_eucl['var'])\n",
    "        matrix_LS_fsva_eucl.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_int_test, pred_fsva_eucl['mu'], pred_fsva_eucl['var'])\n",
    "\n",
    "        pred_fsva_eucl_exp = model_FSVA_eucl.predict(gp_coords_pred = X_exp_test, predict_var = True)\n",
    "\n",
    "        matrix_RMSE_exp_fsva_eucl.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_exp_test - pred_fsva_eucl_exp['mu'])))\n",
    "        matrix_CRPS_exp_fsva_eucl.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_exp_test, pred_fsva_eucl_exp['mu'], pred_fsva_eucl_exp['var'])\n",
    "        matrix_LS_exp_fsva_eucl.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_exp_test, pred_fsva_eucl_exp['mu'], pred_fsva_eucl_exp['var'])\n",
    "\n",
    "        \n",
    "        \n",
    "        start_time = time.time()\n",
    "        # FSVA_corr\n",
    "        model_FSVA_corr = gpb.GPModel(gp_coords=X_train, cov_function=\"matern_ard\", cov_fct_shape = 1.5,num_neighbors = num_vecchia_neighbors,\n",
    "                                 likelihood=\"gaussian\",num_ind_points = num_ind_points,ind_points_selection = \"kmeans++\",\n",
    "                                 matrix_inversion_method = \"cholesky\", gp_approx=\"vecchia\",seed = 4)\n",
    "        model_FSVA_corr.set_optim_params(params={\"init_cov_pars\": result})\n",
    "        model_FSVA_corr.fit(y = y_train)\n",
    "        # End the timer\n",
    "        end_time = time.time()\n",
    "        matrix_t_fsva_corr.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        pred_fsva_corr = model_FSVA_corr.predict(gp_coords_pred = X_int_test, predict_var = True)\n",
    "        end_time = time.time()\n",
    "        matrix_tp_fsva_corr.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "        matrix_RMSE_fsva_corr.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_int_test - pred_fsva_corr['mu'])))\n",
    "        matrix_CRPS_fsva_corr.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_int_test, pred_fsva_corr['mu'], pred_fsva_corr['var'])\n",
    "        matrix_LS_fsva_corr.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_int_test, pred_fsva_corr['mu'], pred_fsva_corr['var'])\n",
    "\n",
    "        pred_fsva_corr_exp = model_FSVA_corr.predict(gp_coords_pred = X_exp_test, predict_var = True)\n",
    "\n",
    "        matrix_RMSE_exp_fsva_corr.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_exp_test - pred_fsva_corr_exp['mu'])))\n",
    "        matrix_CRPS_exp_fsva_corr.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_exp_test, pred_fsva_corr_exp['mu'], pred_fsva_corr_exp['var'])\n",
    "        matrix_LS_exp_fsva_corr.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_exp_test, pred_fsva_corr_exp['mu'], pred_fsva_corr_exp['var'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8227c6",
   "metadata": {},
   "source": [
    "### Matern 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06852279",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0877, 0.4387]])\n",
      "0\n",
      "torch.Size([10000, 2])\n",
      "torch.Size([9916, 2])\n",
      "torch.Size([10000, 2])\n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "1\n",
      "torch.Size([10000, 2])\n",
      "torch.Size([9967, 2])\n",
      "torch.Size([10000, 2])\n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "tensor([[0.1962, 0.2834, 0.3706, 0.4578, 0.5450, 0.6321, 0.7193, 0.8065, 0.8937,\n",
      "         0.9809]])\n",
      "0\n",
      "torch.Size([10000, 10])\n",
      "torch.Size([10015, 10])\n",
      "torch.Size([10000, 10])\n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "1\n",
      "torch.Size([10000, 10])\n",
      "torch.Size([10135, 10])\n",
      "torch.Size([10000, 10])\n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n"
     ]
    }
   ],
   "source": [
    "# number of inducing points\n",
    "num_ind_points = 200\n",
    "# Different number of Vecchia neighbors\n",
    "num_vecchia_neighbors = 30\n",
    "# Number of input dimensions\n",
    "vector_d = [2,10]\n",
    "# number of reps\n",
    "num_reps = 10\n",
    "if toy:\n",
    "    num_reps = 2\n",
    "# Define row and column names\n",
    "row_names = ['2-d','5-d','10-d','20-d','50-d','100-d']\n",
    "col_names = ['Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6',\n",
    "             'Iteration 7', 'Iteration 8', 'Iteration 9', 'Iteration 10']\n",
    "if toy:\n",
    "    row_names = ['2-d','10-d']\n",
    "# List of matrix names\n",
    "matrix_names = [\n",
    "    \"matrix_t_vecchia\", \"matrix_t_fitc\", \"matrix_t_fsva_corr\", \"matrix_t_fsva_eucl\",\n",
    "    \"matrix_tp_vecchia\", \"matrix_tp_fitc\", \"matrix_tp_fsva_corr\", \"matrix_tp_fsva_eucl\",\n",
    "    \"matrix_RMSE_vecchia\", \"matrix_RMSE_fitc\", \"matrix_RMSE_fsva_corr\", \"matrix_RMSE_fsva_eucl\",\n",
    "    \"matrix_LS_vecchia\", \"matrix_LS_fitc\", \"matrix_LS_fsva_corr\", \"matrix_LS_fsva_eucl\",\n",
    "    \"matrix_CRPS_vecchia\", \"matrix_CRPS_fitc\", \"matrix_CRPS_fsva_corr\", \"matrix_CRPS_fsva_eucl\",\n",
    "    \"matrix_RMSE_exp_vecchia\", \"matrix_RMSE_exp_fitc\", \"matrix_RMSE_exp_fsva_corr\", \"matrix_RMSE_exp_fsva_eucl\",\n",
    "    \"matrix_LS_exp_vecchia\", \"matrix_LS_exp_fitc\", \"matrix_LS_exp_fsva_corr\", \"matrix_LS_exp_fsva_eucl\",\n",
    "    \"matrix_CRPS_exp_vecchia\", \"matrix_CRPS_exp_fitc\", \"matrix_CRPS_exp_fsva_corr\", \"matrix_CRPS_exp_fsva_eucl\"\n",
    "]\n",
    "\n",
    "# Create empty matrices and assign them dynamically to the global scope\n",
    "for matrix_name in matrix_names:\n",
    "    globals()[matrix_name] = pd.DataFrame(np.zeros((len(row_names), len(col_names))), \n",
    "                      index=row_names, columns=col_names)\n",
    "\n",
    "# Nested loop to iterate over both vectors\n",
    "for i, val1 in enumerate(vector_d):\n",
    "    # Ranges\n",
    "    ranges = torch.tensor([(0.15, 0.3,0.45,0.6,0.75)])\n",
    "    avg_dist_5 = np.sqrt(5/6)\n",
    "    if val1 != 5:\n",
    "        # average distance\n",
    "        avg_dist = np.sqrt(val1/6)\n",
    "        min_ratio = avg_dist_5/0.15\n",
    "        max_ratio = avg_dist_5/0.75\n",
    "        min_range = avg_dist/min_ratio\n",
    "        max_range = avg_dist/max_ratio\n",
    "        ranges = torch.linspace(min_range, max_range, val1)\n",
    "        ranges = ranges.unsqueeze(0)\n",
    "    ranges = 2.448/2.647 * ranges\n",
    "    print(ranges)\n",
    "    flattened = np.asarray(ranges).reshape(-1)\n",
    "    result = np.concatenate(([0.3, 1], flattened))\n",
    "    for j in range(num_reps):\n",
    "        print(j)\n",
    "        url = f\"https://raw.githubusercontent.com/TimGyger/VIF/refs/heads/main/Simulation/Data/Matern25/simulated_data_Matern25_{val1}_{j}.txt\"\n",
    "\n",
    "        # Load the CSV file directly from the URL\n",
    "        df = pd.read_csv(url)\n",
    "\n",
    "        # Select the first d columns for X\n",
    "        X = df.iloc[:, :val1]  # First d columns\n",
    "\n",
    "        # Select the last column for y\n",
    "        y = df.iloc[:, -1]  # Last column\n",
    "        \n",
    "        # Construct Test and Train sets \n",
    "        corner = 1 - 1/(4**(1/val1))\n",
    "        X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        y = torch.tensor(y.values, dtype=torch.float32)\n",
    "        upper_corner_mask = torch.all(X > corner, dim = 1)\n",
    "        \n",
    "        X_exp_test = X[upper_corner_mask]\n",
    "        y_exp_test =  y[upper_corner_mask]\n",
    "        X_remaining = X[~upper_corner_mask]\n",
    "        y_remaining = y[~upper_corner_mask]\n",
    "        X_int_test = X_remaining[:10000]\n",
    "        y_int_test = y_remaining[:10000]\n",
    "        X_train = X_remaining[10000:20000]\n",
    "        y_train = y_remaining[10000:20000]\n",
    "        \n",
    "        print(X_train.size())\n",
    "        print(X_exp_test.size())\n",
    "        print(X_int_test.size())\n",
    "        \n",
    "        \n",
    "        X_train = np.asarray(X_train)\n",
    "        y_train = np.asarray(y_train)\n",
    "        X_int_test = np.asarray(X_int_test)\n",
    "        y_int_test = np.asarray(y_int_test)\n",
    "        X_exp_test = np.asarray(X_exp_test)\n",
    "        y_exp_test = np.asarray(y_exp_test)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # Vecchia\n",
    "        model_vecchia = gpb.GPModel(gp_coords=X_train, cov_function=\"matern_ard\", cov_fct_shape = 2.5,\n",
    "                                 likelihood=\"gaussian\",num_neighbors = num_vecchia_neighbors,\n",
    "                                 matrix_inversion_method = \"cholesky\", gp_approx=\"vecchia\",seed = 10)\n",
    "        model_vecchia.set_optim_params(params={\"init_cov_pars\": result})\n",
    "        model_vecchia.fit(y = y_train)\n",
    "        # End the timer\n",
    "        end_time = time.time()\n",
    "        matrix_t_vecchia.loc[row_names[i], col_names[j]] = end_time - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        pred_vecchia = model_vecchia.predict(gp_coords_pred = X_int_test, predict_var = True)\n",
    "        end_time = time.time()\n",
    "        matrix_tp_vecchia.loc[row_names[i], col_names[j]] = end_time - start_time\n",
    "        matrix_RMSE_vecchia.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_int_test - pred_vecchia['mu'])))\n",
    "        matrix_CRPS_vecchia.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_int_test, pred_vecchia['mu'], pred_vecchia['var'])\n",
    "        matrix_LS_vecchia.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_int_test, pred_vecchia['mu'], pred_vecchia['var'])\n",
    "\n",
    "        pred_vecchia_exp = model_vecchia.predict(gp_coords_pred = X_exp_test, predict_var = True)\n",
    "\n",
    "        matrix_RMSE_exp_vecchia.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_exp_test - pred_vecchia_exp['mu'])))\n",
    "        matrix_CRPS_exp_vecchia.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_exp_test, pred_vecchia_exp['mu'], pred_vecchia_exp['var'])\n",
    "        matrix_LS_exp_vecchia.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_exp_test, pred_vecchia_exp['mu'], pred_vecchia_exp['var'])\n",
    "\n",
    "        start_time = time.time()\n",
    "        # FITC\n",
    "        model_FITC = gpb.GPModel(gp_coords=X_train, cov_function=\"matern_ard\", cov_fct_shape = 2.5,\n",
    "                                 likelihood=\"gaussian\",num_ind_points = 200,ind_points_selection = \"kmeans++\",\n",
    "                                 matrix_inversion_method = \"cholesky\", gp_approx=\"fitc\",seed = 10)\n",
    "        \n",
    "        model_FITC.set_optim_params(params={\"init_cov_pars\": result})\n",
    "        \n",
    "        model_FITC.fit(y = y_train)\n",
    "        # End the timer\n",
    "        end_time = time.time()\n",
    "        matrix_t_fitc.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        pred_fitc = model_FITC.predict(gp_coords_pred = X_int_test, predict_var = True)\n",
    "        end_time = time.time()\n",
    "        matrix_tp_fitc.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "        matrix_RMSE_fitc.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_int_test - pred_fitc['mu'])))\n",
    "        matrix_CRPS_fitc.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_int_test, pred_fitc['mu'], pred_fitc['var'])\n",
    "        matrix_LS_fitc.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_int_test, pred_fitc['mu'], pred_fitc['var'])\n",
    "\n",
    "        pred_fitc_exp = model_FITC.predict(gp_coords_pred = X_exp_test, predict_var = True)\n",
    "\n",
    "        matrix_RMSE_exp_fitc.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_exp_test - pred_fitc_exp['mu'])))\n",
    "        matrix_CRPS_exp_fitc.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_exp_test, pred_fitc_exp['mu'], pred_fitc_exp['var'])\n",
    "        matrix_LS_exp_fitc.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_exp_test, pred_fitc_exp['mu'], pred_fitc_exp['var'])\n",
    "\n",
    "        start_time = time.time()\n",
    "        # FSVA_eucl\n",
    "        model_FSVA_eucl = gpb.GPModel(gp_coords=X_train, cov_function=\"matern_ard\", cov_fct_shape = 2.5,num_neighbors = num_vecchia_neighbors,\n",
    "                                 likelihood=\"gaussian\",num_ind_points = num_ind_points,ind_points_selection = \"kmeans++\",\n",
    "                                 matrix_inversion_method = \"cholesky\", gp_approx=\"vecchia\",seed = 2)\n",
    "        model_FSVA_eucl.set_optim_params(params={\"init_cov_pars\": result})\n",
    "        model_FSVA_eucl.fit(y = y_train)\n",
    "        # End the timer\n",
    "        end_time = time.time()\n",
    "        matrix_t_fsva_eucl.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        pred_fsva_eucl = model_FSVA_eucl.predict(gp_coords_pred = X_int_test, predict_var = True)\n",
    "        end_time = time.time()\n",
    "        matrix_tp_fsva_eucl.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "        matrix_RMSE_fsva_eucl.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_int_test - pred_fsva_eucl['mu'])))\n",
    "        matrix_CRPS_fsva_eucl.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_int_test, pred_fsva_eucl['mu'], pred_fsva_eucl['var'])\n",
    "        matrix_LS_fsva_eucl.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_int_test, pred_fsva_eucl['mu'], pred_fsva_eucl['var'])\n",
    "\n",
    "        pred_fsva_eucl_exp = model_FSVA_eucl.predict(gp_coords_pred = X_exp_test, predict_var = True)\n",
    "\n",
    "        matrix_RMSE_exp_fsva_eucl.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_exp_test - pred_fsva_eucl_exp['mu'])))\n",
    "        matrix_CRPS_exp_fsva_eucl.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_exp_test, pred_fsva_eucl_exp['mu'], pred_fsva_eucl_exp['var'])\n",
    "        matrix_LS_exp_fsva_eucl.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_exp_test, pred_fsva_eucl_exp['mu'], pred_fsva_eucl_exp['var'])\n",
    "\n",
    "        \n",
    "        \n",
    "        start_time = time.time()\n",
    "        # FSVA_corr\n",
    "        model_FSVA_corr = gpb.GPModel(gp_coords=X_train, cov_function=\"matern_ard\", cov_fct_shape = 2.5,num_neighbors = num_vecchia_neighbors,\n",
    "                                 likelihood=\"gaussian\",num_ind_points = num_ind_points,ind_points_selection = \"kmeans++\",\n",
    "                                 matrix_inversion_method = \"cholesky\", gp_approx=\"vecchia\",seed = 4)\n",
    "        model_FSVA_corr.set_optim_params(params={\"init_cov_pars\": result})\n",
    "        model_FSVA_corr.fit(y = y_train)\n",
    "        # End the timer\n",
    "        end_time = time.time()\n",
    "        matrix_t_fsva_corr.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        pred_fsva_corr = model_FSVA_corr.predict(gp_coords_pred = X_int_test, predict_var = True)\n",
    "        end_time = time.time()\n",
    "        matrix_tp_fsva_corr.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "        matrix_RMSE_fsva_corr.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_int_test - pred_fsva_corr['mu'])))\n",
    "        matrix_CRPS_fsva_corr.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_int_test, pred_fsva_corr['mu'], pred_fsva_corr['var'])\n",
    "        matrix_LS_fsva_corr.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_int_test, pred_fsva_corr['mu'], pred_fsva_corr['var'])\n",
    "\n",
    "        pred_fsva_corr_exp = model_FSVA_corr.predict(gp_coords_pred = X_exp_test, predict_var = True)\n",
    "\n",
    "        matrix_RMSE_exp_fsva_corr.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_exp_test - pred_fsva_corr_exp['mu'])))\n",
    "        matrix_CRPS_exp_fsva_corr.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_exp_test, pred_fsva_corr_exp['mu'], pred_fsva_corr_exp['var'])\n",
    "        matrix_LS_exp_fsva_corr.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_exp_test, pred_fsva_corr_exp['mu'], pred_fsva_corr_exp['var'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e405aefe",
   "metadata": {},
   "source": [
    "### Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96a43393",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0949, 0.4743]])\n",
      "0\n",
      "torch.Size([10000, 2])\n",
      "torch.Size([9916, 2])\n",
      "torch.Size([10000, 2])\n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "1\n",
      "torch.Size([10000, 2])\n",
      "torch.Size([9967, 2])\n",
      "torch.Size([10000, 2])\n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "tensor([[0.2121, 0.3064, 0.4007, 0.4950, 0.5893, 0.6835, 0.7778, 0.8721, 0.9664,\n",
      "         1.0607]])\n",
      "0\n",
      "torch.Size([10000, 10])\n",
      "torch.Size([10015, 10])\n",
      "torch.Size([10000, 10])\n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Warning] There is no intercept for modeling a possibly non-zero mean of the random effects. Consider including an intercept (= a column of 1's) in the covariates 'X' \n",
      "1\n",
      "torch.Size([10000, 10])\n",
      "torch.Size([10135, 10])\n",
      "torch.Size([10000, 10])\n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n",
      "[GPBoost] [Info] Starting kmeans++ algorithm for determining inducing points \n",
      "[GPBoost] [Info] Inducing points have been determined \n"
     ]
    }
   ],
   "source": [
    "# number of inducing points\n",
    "num_ind_points = 200\n",
    "# Different number of Vecchia neighbors\n",
    "num_vecchia_neighbors = 30\n",
    "# Number of input dimensions\n",
    "vector_d = [2,10]\n",
    "# number of reps\n",
    "num_reps = 10\n",
    "if toy:\n",
    "    num_reps = 2\n",
    "# Define row and column names\n",
    "row_names = ['2-d','10-d']\n",
    "col_names = ['Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6',\n",
    "             'Iteration 7', 'Iteration 8', 'Iteration 9', 'Iteration 10']\n",
    "\n",
    "# List of matrix names\n",
    "matrix_names = [\n",
    "    \"matrix_t_vecchia\", \"matrix_t_fitc\", \"matrix_t_fsva_corr\", \"matrix_t_fsva_eucl\",\n",
    "    \"matrix_tp_vecchia\", \"matrix_tp_fitc\", \"matrix_tp_fsva_corr\", \"matrix_tp_fsva_eucl\",\n",
    "    \"matrix_RMSE_vecchia\", \"matrix_RMSE_fitc\", \"matrix_RMSE_fsva_corr\", \"matrix_RMSE_fsva_eucl\",\n",
    "    \"matrix_LS_vecchia\", \"matrix_LS_fitc\", \"matrix_LS_fsva_corr\", \"matrix_LS_fsva_eucl\",\n",
    "    \"matrix_CRPS_vecchia\", \"matrix_CRPS_fitc\", \"matrix_CRPS_fsva_corr\", \"matrix_CRPS_fsva_eucl\",\n",
    "    \"matrix_RMSE_exp_vecchia\", \"matrix_RMSE_exp_fitc\", \"matrix_RMSE_exp_fsva_corr\", \"matrix_RMSE_exp_fsva_eucl\",\n",
    "    \"matrix_LS_exp_vecchia\", \"matrix_LS_exp_fitc\", \"matrix_LS_exp_fsva_corr\", \"matrix_LS_exp_fsva_eucl\",\n",
    "    \"matrix_CRPS_exp_vecchia\", \"matrix_CRPS_exp_fitc\", \"matrix_CRPS_exp_fsva_corr\", \"matrix_CRPS_exp_fsva_eucl\"\n",
    "]\n",
    "\n",
    "# Create empty matrices and assign them dynamically to the global scope\n",
    "for matrix_name in matrix_names:\n",
    "    globals()[matrix_name] = pd.DataFrame(np.zeros((len(row_names), len(col_names))), \n",
    "                      index=row_names, columns=col_names)\n",
    "\n",
    "# Nested loop to iterate over both vectors\n",
    "for i, val1 in enumerate(vector_d):\n",
    "    # Ranges\n",
    "    ranges = torch.tensor([(0.15, 0.3,0.45,0.6,0.75)])\n",
    "    avg_dist_5 = np.sqrt(5/6)\n",
    "    if val1 != 5:\n",
    "        # average distance\n",
    "        avg_dist = np.sqrt(val1/6)\n",
    "        min_ratio = avg_dist_5/0.15\n",
    "        max_ratio = avg_dist_5/0.75\n",
    "        min_range = avg_dist/min_ratio\n",
    "        max_range = avg_dist/max_ratio\n",
    "        ranges = torch.linspace(min_range, max_range, val1)\n",
    "        ranges = ranges.unsqueeze(0)\n",
    "        \n",
    "    print(ranges)\n",
    "    flattened = np.asarray(ranges).reshape(-1)\n",
    "    result = np.concatenate(([0.3, 1], flattened))\n",
    "    for j in range(num_reps):\n",
    "        print(j)\n",
    "        url = f\"https://raw.githubusercontent.com/TimGyger/VIF/refs/heads/main/Simulation/Data/Gaussian/simulated_data_Gaussian_{val1}_{j}.txt\"\n",
    "\n",
    "        # Load the CSV file directly from the URL\n",
    "        df = pd.read_csv(url)\n",
    "\n",
    "        # Select the first d columns for X\n",
    "        X = df.iloc[:, :val1]  # First d columns\n",
    "\n",
    "        # Select the last column for y\n",
    "        y = df.iloc[:, -1]  # Last column\n",
    "        \n",
    "        # Construct Test and Train sets \n",
    "        corner = 1 - 1/(4**(1/val1))\n",
    "        X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        y = torch.tensor(y.values, dtype=torch.float32)\n",
    "        upper_corner_mask = torch.all(X > corner, dim = 1)\n",
    "        \n",
    "        X_exp_test = X[upper_corner_mask]\n",
    "        y_exp_test =  y[upper_corner_mask]\n",
    "        X_remaining = X[~upper_corner_mask]\n",
    "        y_remaining = y[~upper_corner_mask]\n",
    "        X_int_test = X_remaining[:10000]\n",
    "        y_int_test = y_remaining[:10000]\n",
    "        X_train = X_remaining[10000:20000]\n",
    "        y_train = y_remaining[10000:20000]\n",
    "        \n",
    "        print(X_train.size())\n",
    "        print(X_exp_test.size())\n",
    "        print(X_int_test.size())\n",
    "        \n",
    "        \n",
    "        X_train = np.asarray(X_train)\n",
    "        y_train = np.asarray(y_train)\n",
    "        X_int_test = np.asarray(X_int_test)\n",
    "        y_int_test = np.asarray(y_int_test)\n",
    "        X_exp_test = np.asarray(X_exp_test)\n",
    "        y_exp_test = np.asarray(y_exp_test)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # Vecchia\n",
    "        model_vecchia = gpb.GPModel(gp_coords=X_train, cov_function=\"gaussian_ard\", \n",
    "                                 likelihood=\"gaussian\",num_neighbors = num_vecchia_neighbors,\n",
    "                                 matrix_inversion_method = \"cholesky\", gp_approx=\"vecchia\",seed = 10)\n",
    "        model_vecchia.set_optim_params(params={\"init_cov_pars\": result})\n",
    "        model_vecchia.fit(y = y_train)\n",
    "        # End the timer\n",
    "        end_time = time.time()\n",
    "        matrix_t_vecchia.loc[row_names[i], col_names[j]] = end_time - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        pred_vecchia = model_vecchia.predict(gp_coords_pred = X_int_test, predict_var = True)\n",
    "        end_time = time.time()\n",
    "        matrix_tp_vecchia.loc[row_names[i], col_names[j]] = end_time - start_time\n",
    "        matrix_RMSE_vecchia.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_int_test - pred_vecchia['mu'])))\n",
    "        matrix_CRPS_vecchia.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_int_test, pred_vecchia['mu'], pred_vecchia['var'])\n",
    "        matrix_LS_vecchia.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_int_test, pred_vecchia['mu'], pred_vecchia['var'])\n",
    "\n",
    "        pred_vecchia_exp = model_vecchia.predict(gp_coords_pred = X_exp_test, predict_var = True)\n",
    "\n",
    "        matrix_RMSE_exp_vecchia.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_exp_test - pred_vecchia_exp['mu'])))\n",
    "        matrix_CRPS_exp_vecchia.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_exp_test, pred_vecchia_exp['mu'], pred_vecchia_exp['var'])\n",
    "        matrix_LS_exp_vecchia.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_exp_test, pred_vecchia_exp['mu'], pred_vecchia_exp['var'])\n",
    "\n",
    "        start_time = time.time()\n",
    "        # FITC\n",
    "        model_FITC = gpb.GPModel(gp_coords=X_train, cov_function=\"gaussian_ard\", \n",
    "                                 likelihood=\"gaussian\",num_ind_points = 200,ind_points_selection = \"kmeans++\",\n",
    "                                 matrix_inversion_method = \"cholesky\", gp_approx=\"fitc\",seed = 10)\n",
    "        \n",
    "        model_FITC.set_optim_params(params={\"init_cov_pars\": result})\n",
    "        \n",
    "        model_FITC.fit(y = y_train)\n",
    "        # End the timer\n",
    "        end_time = time.time()\n",
    "        matrix_t_fitc.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        pred_fitc = model_FITC.predict(gp_coords_pred = X_int_test, predict_var = True)\n",
    "        end_time = time.time()\n",
    "        matrix_tp_fitc.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "        matrix_RMSE_fitc.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_int_test - pred_fitc['mu'])))\n",
    "        matrix_CRPS_fitc.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_int_test, pred_fitc['mu'], pred_fitc['var'])\n",
    "        matrix_LS_fitc.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_int_test, pred_fitc['mu'], pred_fitc['var'])\n",
    "\n",
    "        pred_fitc_exp = model_FITC.predict(gp_coords_pred = X_exp_test, predict_var = True)\n",
    "\n",
    "        matrix_RMSE_exp_fitc.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_exp_test - pred_fitc_exp['mu'])))\n",
    "        matrix_CRPS_exp_fitc.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_exp_test, pred_fitc_exp['mu'], pred_fitc_exp['var'])\n",
    "        matrix_LS_exp_fitc.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_exp_test, pred_fitc_exp['mu'], pred_fitc_exp['var'])\n",
    "\n",
    "        start_time = time.time()\n",
    "        # FSVA_eucl\n",
    "        model_FSVA_eucl = gpb.GPModel(gp_coords=X_train, cov_function=\"gaussian_ard\", num_neighbors = num_vecchia_neighbors,\n",
    "                                 likelihood=\"gaussian\",num_ind_points = num_ind_points,ind_points_selection = \"kmeans++\",\n",
    "                                 matrix_inversion_method = \"cholesky\", gp_approx=\"vecchia\",seed = 2)\n",
    "        model_FSVA_eucl.set_optim_params(params={\"init_cov_pars\": result})\n",
    "        model_FSVA_eucl.fit(y = y_train)\n",
    "        # End the timer\n",
    "        end_time = time.time()\n",
    "        matrix_t_fsva_eucl.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        pred_fsva_eucl = model_FSVA_eucl.predict(gp_coords_pred = X_int_test, predict_var = True)\n",
    "        end_time = time.time()\n",
    "        matrix_tp_fsva_eucl.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "        matrix_RMSE_fsva_eucl.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_int_test - pred_fsva_eucl['mu'])))\n",
    "        matrix_CRPS_fsva_eucl.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_int_test, pred_fsva_eucl['mu'], pred_fsva_eucl['var'])\n",
    "        matrix_LS_fsva_eucl.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_int_test, pred_fsva_eucl['mu'], pred_fsva_eucl['var'])\n",
    "\n",
    "        pred_fsva_eucl_exp = model_FSVA_eucl.predict(gp_coords_pred = X_exp_test, predict_var = True)\n",
    "\n",
    "        matrix_RMSE_exp_fsva_eucl.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_exp_test - pred_fsva_eucl_exp['mu'])))\n",
    "        matrix_CRPS_exp_fsva_eucl.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_exp_test, pred_fsva_eucl_exp['mu'], pred_fsva_eucl_exp['var'])\n",
    "        matrix_LS_exp_fsva_eucl.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_exp_test, pred_fsva_eucl_exp['mu'], pred_fsva_eucl_exp['var'])\n",
    "\n",
    "        \n",
    "        \n",
    "        start_time = time.time()\n",
    "        # FSVA_corr\n",
    "        model_FSVA_corr = gpb.GPModel(gp_coords=X_train, cov_function=\"gaussian_ard\", num_neighbors = num_vecchia_neighbors,\n",
    "                                 likelihood=\"gaussian\",num_ind_points = num_ind_points,ind_points_selection = \"kmeans++\",\n",
    "                                 matrix_inversion_method = \"cholesky\", gp_approx=\"vecchia\",seed = 4)\n",
    "        model_FSVA_corr.set_optim_params(params={\"init_cov_pars\": result})\n",
    "        model_FSVA_corr.fit(y = y_train)\n",
    "        # End the timer\n",
    "        end_time = time.time()\n",
    "        matrix_t_fsva_corr.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        pred_fsva_corr = model_FSVA_corr.predict(gp_coords_pred = X_int_test, predict_var = True)\n",
    "        end_time = time.time()\n",
    "        matrix_tp_fsva_corr.loc[row_names[i], col_names[j]]  = end_time - start_time\n",
    "        matrix_RMSE_fsva_corr.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_int_test - pred_fsva_corr['mu'])))\n",
    "        matrix_CRPS_fsva_corr.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_int_test, pred_fsva_corr['mu'], pred_fsva_corr['var'])\n",
    "        matrix_LS_fsva_corr.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_int_test, pred_fsva_corr['mu'], pred_fsva_corr['var'])\n",
    "\n",
    "        pred_fsva_corr_exp = model_FSVA_corr.predict(gp_coords_pred = X_exp_test, predict_var = True)\n",
    "\n",
    "        matrix_RMSE_exp_fsva_corr.loc[row_names[i], col_names[j]]  = np.sqrt(np.mean(np.square(y_exp_test - pred_fsva_corr_exp['mu'])))\n",
    "        matrix_CRPS_exp_fsva_corr.loc[row_names[i], col_names[j]]  = crps_norm_vectorized(y_exp_test, pred_fsva_corr_exp['mu'], pred_fsva_corr_exp['var'])\n",
    "        matrix_LS_exp_fsva_corr.loc[row_names[i], col_names[j]]  = log_score_norm_vectorized(y_exp_test, pred_fsva_corr_exp['mu'], pred_fsva_corr_exp['var'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28ccfdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (FSA_env)",
   "language": "python",
   "name": "fsa_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
